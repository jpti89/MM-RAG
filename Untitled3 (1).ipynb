{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSAaeofZ5W6j"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from llama_index.core.evaluation import MultiModalRetrieverEvaluator\n",
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from llama_index.core.evaluation import EvaluationResult\n",
        "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
        "\n",
        "# Define retriever evaluators\n",
        "clip_evaluator = MultiModalRetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=clip_retriever\n",
        ")\n",
        "\n",
        "text_desc_evaluator = MultiModalRetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=text_desc_retriever\n",
        ")\n",
        "\n",
        "# Create labeled datasets for evaluation\n",
        "image_qa_dataset = create_labelled_retrieval_dataset(\n",
        "    r\"(?:([A-Z]+).jpg)\", image_nodes, \"image\"\n",
        ")\n",
        "\n",
        "text_qa_dataset = create_labelled_retrieval_dataset(\n",
        "    r\"(?:To sign ([A-Z]+) in ASL:)\", text_nodes, \"text\"\n",
        ")\n",
        "\n",
        "text_desc_qa_dataset = create_labelled_retrieval_dataset(\n",
        "    r\"(?:([A-Z]+).jpg)\", image_with_text_nodes, \"image\"\n",
        ")\n",
        "\n",
        "# Evaluate the retrievers on the datasets\n",
        "eval_results_image = await clip_evaluator.evaluate_dataset(image_qa_dataset)\n",
        "eval_results_text = await clip_evaluator.evaluate_dataset(text_qa_dataset)\n",
        "eval_results_text_desc = await text_desc_evaluator.evaluate_dataset(text_desc_qa_dataset)\n",
        "\n",
        "# Convert evaluation results to DataFrame\n",
        "results_df = get_retrieval_results_df(\n",
        "    names=[\"image_retrieval\", \"text_retrieval\", \"text_desc_retrieval\"],\n",
        "    results_arr=[\n",
        "        eval_results_image,\n",
        "        eval_results_text,\n",
        "        eval_results_text_desc,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Load human-labeled ground-truth data\n",
        "with open(\"asl_data/human_responses.json\") as json_file:\n",
        "    human_answers = json.load(json_file)\n",
        "\n",
        "# Load or evaluate responses\n",
        "load_previous_responses = True\n",
        "number_evals = 27\n",
        "\n",
        "if not load_previous_responses:\n",
        "    response_data = []\n",
        "    for letter in tqdm.tqdm(asl_text_descriptions.keys()):\n",
        "        data_entry = {}\n",
        "        query = QUERY_STR_TEMPLATE.format(symbol=letter)\n",
        "        data_entry[\"query\"] = query\n",
        "\n",
        "        responses = {}\n",
        "        for name, engine in rag_engines.items():\n",
        "            this_response = {}\n",
        "            result = engine.query(query)\n",
        "            this_response[\"response\"] = result.response\n",
        "\n",
        "            sources = {}\n",
        "            source_image_nodes = []\n",
        "            source_text_nodes = []\n",
        "\n",
        "            # image sources\n",
        "            source_image_nodes = [\n",
        "                score_img_node.node.metadata[\"file_path\"]\n",
        "                for score_img_node in result.metadata[\"image_nodes\"]\n",
        "            ]\n",
        "\n",
        "            # text sources\n",
        "            source_text_nodes = [\n",
        "                score_text_node.node.text\n",
        "                for score_text_node in result.metadata[\"text_nodes\"]\n",
        "            ]\n",
        "\n",
        "            sources[\"images\"] = source_image_nodes\n",
        "            sources[\"texts\"] = source_text_nodes\n",
        "            this_response[\"sources\"] = sources\n",
        "\n",
        "            responses[name] = this_response\n",
        "        data_entry[\"responses\"] = responses\n",
        "        response_data.append(data_entry)\n",
        "\n",
        "    # Save responses\n",
        "    with open(\"asl_data/response_data.json\", \"w\") as json_file:\n",
        "        json.dump(response_data, json_file)\n",
        "else:\n",
        "    # Load previous responses\n",
        "    with open(\"asl_data/response_data.json\") as json_file:\n",
        "        response_data = json.load(json_file)\n",
        "\n",
        "# Define judges for evaluation\n",
        "judges = {}\n",
        "judges[\"correctness\"] = CorrectnessEvaluator(\n",
        "    llm=OpenAI(temperature=0, model=\"gpt-4\"),\n",
        ")\n",
        "judges[\"relevancy\"] = MultiModalRelevancyEvaluator(\n",
        "    multi_modal_llm=OpenAIMultiModal(\n",
        "        model=\"gpt-4-vision-preview\",\n",
        "        max_new_tokens=300,\n",
        "    )\n",
        ")\n",
        "judges[\"faithfulness\"] = MultiModalFaithfulnessEvaluator(\n",
        "    multi_modal_llm=OpenAIMultiModal(\n",
        "        model=\"gpt-4-vision-preview\",\n",
        "        max_new_tokens=300,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Load or evaluate previous evaluations\n",
        "load_previous_evaluations = True\n",
        "\n",
        "if not load_previous_evaluations:\n",
        "    evals = {\n",
        "        \"names\": [],\n",
        "        \"correctness\": [],\n",
        "        \"relevancy\": [],\n",
        "        \"faithfulness\": [],\n",
        "    }\n",
        "\n",
        "    # Loop through responses and evaluate them\n",
        "    for data_entry in tqdm.tqdm(response_data[:number_evals]):\n",
        "        reg_ex = r\"(?:How can I sign a ([A-Z]+)?)\"\n",
        "        match = re.search(reg_ex, data_entry[\"query\"])\n",
        "\n",
        "        batch_names = []\n",
        "        batch_correctness = []\n",
        "        batch_relevancy = []\n",
        "        batch_faithfulness = []\n",
        "        if match:\n",
        "            letter = match.group(1)\n",
        "            reference_answer = human_answers[letter]\n",
        "            for rag_name, rag_response_data in data_entry[\"responses\"].items():\n",
        "                correctness_result = await judges[\"correctness\"].evaluate(\n",
        "                    query=data_entry[\"query\"],\n",
        "                    response=rag_response_data[\"response\"],\n",
        "                    reference=reference_answer,\n",
        "                )\n",
        "\n",
        "                relevancy_result = judges[\"relevancy\"].evaluate(\n",
        "                    query=data_entry[\"query\"],\n",
        "                    response=rag_response_data[\"response\"],\n",
        "                    contexts=rag_response_data[\"sources\"][\"texts\"],\n",
        "                    image_paths=rag_response_data[\"sources\"][\"images\"],\n",
        "                )\n",
        "\n",
        "                faithfulness_result = judges[\"faithfulness\"].evaluate(\n",
        "                    query=data_entry[\"query\"],\n",
        "                    response=rag_response_data[\"response\"],\n",
        "                    contexts=rag_response_data[\"sources\"][\"texts\"],\n",
        "                    image_paths=rag_response_data[\"sources\"][\"images\"],\n",
        "                )\n",
        "\n",
        "                batch_names.append(rag_name)\n",
        "                batch_correctness.append(correctness_result)\n",
        "                batch_relevancy.append(relevancy_result)\n",
        "                batch_faithfulness.append(faithfulness_result)\n",
        "\n",
        "            evals[\"names\"] += batch_names\n",
        "            evals[\"correctness\"] += batch_correctness\n",
        "            evals[\"relevancy\"] += batch_relevancy\n",
        "            evals[\"faithfulness\"] += batch_faithfulness\n",
        "\n",
        "    # Save evaluations\n",
        "    evaluations_objects = {\n",
        "        \"names\": evals[\"names\"],\n",
        "        \"correctness\": [e.dict() for e in evals[\"correctness\"]],\n",
        "        \"faithfulness\": [e.dict() for e in evals[\"faithfulness\"]],\n",
        "        \"relevancy\": [e.dict() for e in evals[\"relevancy\"]],\n",
        "    }\n",
        "    with open(\"asl_data/evaluations.json\", \"w\") as json_file:\n",
        "        json.dump(evaluations_objects, json_file)\n",
        "else:\n",
        "    # Load previous evaluations\n",
        "    with open(\"asl_data/evaluations.json\") as json_file:\n",
        "        evaluations_objects = json.load(json_file)\n",
        "\n",
        "    evals = {}\n",
        "    evals[\"names\"] = evaluations_objects[\"names\"]\n",
        "    evals[\"correctness\"] = [\n",
        "        EvaluationResult.parse_obj(e)\n",
        "        for e in evaluations_objects[\"correctness\"]\n",
        "    ]\n",
        "    evals[\"faithfulness\"] = [\n",
        "        EvaluationResult.parse_obj(e)\n",
        "        for e in evaluations_objects[\"faithfulness\"]\n",
        "    ]\n",
        "    evals[\"relevancy\"] = [\n",
        "        EvaluationResult.parse_obj(e) for e in evaluations_objects[\"relevancy\"]\n",
        "    ]\n",
        "\n",
        "# Get evaluation results DataFrame\n",
        "deep_eval_df, mean_correctness_df = get_eval_results_df(\n",
        "    evals[\"names\"], evals[\"correctness\"], metric=\"correctness\"\n",
        ")\n",
        "_, mean_relevancy_df = get_eval_results_df(\n",
        "    evals[\"names\"], evals[\"relevancy\"], metric=\"relevancy\"\n",
        ")\n",
        "_, mean_faithfulness_df = get_eval_results_df(\n",
        "    evals[\"names\"], evals[\"faithfulness\"], metric=\"faithfulness\"\n",
        ")\n",
        "\n",
        "mean_scores_df = pd.concat(\n",
        "    [\n",
        "        mean_correctness_df.reset_index(),\n",
        "        mean_relevancy_df.reset_index(),\n",
        "        mean_faithfulness_df.reset_index(),\n",
        "    ],\n",
        "    axis=0,\n",
        "    ignore_index=True,\n",
        ")\n",
        "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
        "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
        "\n",
        "print(deep_eval_df[:4])\n",
        "mean_scores_df\n"
      ]
    }
  ]
}